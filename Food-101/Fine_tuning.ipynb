{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:07.782320Z","iopub.execute_input":"2023-06-06T14:09:07.782908Z","iopub.status.idle":"2023-06-06T14:09:07.791079Z","shell.execute_reply.started":"2023-06-06T14:09:07.782869Z","shell.execute_reply":"2023-06-06T14:09:07.790009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare_historys(original_history, new_history, initial_epochs=5):\n    \"\"\"\n    Compares two model history objects.\n    \"\"\"\n    # Get original history measurements\n    acc = original_history.history[\"accuracy\"]\n    loss = original_history.history[\"loss\"]\n\n    print(len(acc))\n\n    val_acc = original_history.history[\"val_accuracy\"]\n    val_loss = original_history.history[\"val_loss\"]\n\n    # Combine original history with new history\n    total_acc = acc + new_history.history[\"accuracy\"]\n    total_loss = loss + new_history.history[\"loss\"]\n\n    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n\n    print(len(total_acc))\n    print(total_acc)\n\n    # Make plots\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(total_acc, label='Training Accuracy')\n    plt.plot(total_val_acc, label='Validation Accuracy')\n    plt.plot([initial_epochs-1, initial_epochs-1],\n              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(total_loss, label='Training Loss')\n    plt.plot(total_val_loss, label='Validation Loss')\n    plt.plot([initial_epochs-1, initial_epochs-1],\n              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:09:07.821938Z","iopub.execute_input":"2023-06-06T14:09:07.822223Z","iopub.status.idle":"2023-06-06T14:09:07.837716Z","shell.execute_reply.started":"2023-06-06T14:09:07.822199Z","shell.execute_reply":"2023-06-06T14:09:07.836812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 05. Transfer Learning with TensorFlow Part 2: Fine-tuning\n\nIn the previous section, we saw how we could leverage feature extraction transfer learning to get far better results on our Food Vision project than building our own models (even with less data).\n\nNow we're going to cover another type of transfer learning: fine-tuning.\n\nIn **fine-tuning transfer learning** the pre-trained model weights from another model are unfrozen and tweaked during to better suit your own data.\n\nFor feature extraction transfer learning, you may only train the top 1-3 layers of a pre-trained model with your own data, in fine-tuning transfer learning, you might train 1-3+ layers of a pre-trained model (where the '+' indicates that many or all of the layers could be trained).\n\n![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-transfer-learning-feature-extraction-vs-fine-tuning.png)\n*Feature extraction transfer learning vs. fine-tuning transfer learning. The main difference between the two is that in fine-tuning, more layers of the pre-trained model get unfrozen and tuned on custom data. This fine-tuning usually takes more data than feature extraction to be effective.*\n\n## What we're going to cover\n\nWe're going to go through the follow with TensorFlow:\n\n- Introduce fine-tuning, a type of transfer learning to modify a pre-trained model to be more suited to your data\n- Using the Keras Functional API (a differnt way to build models in Keras)\n- Using a smaller dataset to experiment faster (e.g. 1-10% of training samples of 10 classes of food)\n- Data augmentation (how to make your training dataset more diverse without adding more data)\n- Running a series of modelling experiments on our Food Vision data\n  - Model 0: a transfer learning model using the Keras Functional API\n  - Model 1: a feature extraction transfer learning model on 1% of the data with data augmentation\n  - Model 2: a feature extraction transfer learning model on 10% of the data with data augmentation\n  - Model 3: a fine-tuned transfer learning model on 10% of the data\n  - Model 4: a fine-tuned transfer learning model on 100% of the data\n- Introduce the ModelCheckpoint callback to save intermediate training results\n- Compare model experiments results using TensorBoard\n\n## How you can use this notebook\n\nYou can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.\n\nWrite all of the code yourself.\n\nYes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?\n\nYou don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.\n\nDon't worry if you make mistakes, we all do. The way to get better and make less mistakes is to **write more code**.","metadata":{}},{"cell_type":"code","source":"# Are we using a GPU? (if not & you're using Google Colab, go to Runtime -> Change Runtime Type -> Harware Accelerator: GPU )\n!nvidia-smi","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:07.861838Z","iopub.execute_input":"2023-06-06T14:09:07.862102Z","iopub.status.idle":"2023-06-06T14:09:08.847608Z","shell.execute_reply.started":"2023-06-06T14:09:07.862078Z","shell.execute_reply":"2023-06-06T14:09:08.846476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating helper functions\n\nThroughout your machine learning experiments, you'll likely come across snippets of code you want to use over and over again.\n\nFor example, a plotting function which plots a model's `history` object (see `plot_loss_curves()` below).\n\nYou could recreate these functions over and over again.\n\nBut as you might've guessed, rewritting the same functions becomes tedious.\n\nOne of the solutions is to store them in a helper script such as [`helper_functions.py`](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py). And then import the necesary functionality when you need it.\n\nFor example, you might write:\n\n```\nfrom helper_functions import plot_loss_curves\n\n...\n\nplot_loss_curves(history)\n```\n\nLet's see what this looks like.","metadata":{}},{"cell_type":"code","source":"# Get helper_functions.py script from course GitHub\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py \n\n# Import helper functions we're going to use\nfrom helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:08.850220Z","iopub.execute_input":"2023-06-06T14:09:08.850628Z","iopub.status.idle":"2023-06-06T14:09:20.084390Z","shell.execute_reply.started":"2023-06-06T14:09:08.850578Z","shell.execute_reply":"2023-06-06T14:09:20.083468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wonderful, now we've got a bunch of helper functions we can use throughout the notebook without having to rewrite them from scratch each time.\n\n> ðŸ”‘ **Note:** If you're running this notebook in Google Colab, when it times out Colab will delete the `helper_functions.py` file. So to use the functions imported above, you'll have to rerun the cell.","metadata":{}},{"cell_type":"markdown","source":"## 10 Food Classes: Working with less data\n\nWe saw in the [previous notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb) that we could get great results with only 10% of the training data using transfer learning with TensorFlow Hub.\n\nIn this notebook, we're going to continue to work with smaller subsets of the data, except this time we'll have a look at how we can use the in-built pretrained models within the `tf.keras.applications` module as well as how to fine-tune them to our own custom dataset.\n\nWe'll also practice using a new but similar dataloader function to what we've used before, [`image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) which is part of the [`tf.keras.preprocessing`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing) module.\n\nFinally, we'll also be practicing using the [Keras Functional API](https://keras.io/guides/functional_api/) for building deep learning models. The Functional API is a more flexible way to create models than the tf.keras.Sequential API.\n\nWe'll explore each of these in more detail as we go.\n\nLet's start by downloading some data.","metadata":{}},{"cell_type":"code","source":"# Get 10% of the data of the 10 classes\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip \n\nunzip_data(\"10_food_classes_10_percent.zip\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:20.085883Z","iopub.execute_input":"2023-06-06T14:09:20.086875Z","iopub.status.idle":"2023-06-06T14:09:23.277450Z","shell.execute_reply.started":"2023-06-06T14:09:20.086838Z","shell.execute_reply":"2023-06-06T14:09:23.276301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset we're downloading is the 10 food classes dataset (from Food 101) with 10% of the training images we used in the previous notebook.\n\n> ðŸ”‘ **Note:** You can see how this dataset was created in the [image data modification notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb).\n\n","metadata":{}},{"cell_type":"code","source":"# Walk through 10 percent data directory and list number of files\nwalk_through_dir(\"10_food_classes_10_percent\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:23.280850Z","iopub.execute_input":"2023-06-06T14:09:23.281552Z","iopub.status.idle":"2023-06-06T14:09:23.295067Z","shell.execute_reply.started":"2023-06-06T14:09:23.281512Z","shell.execute_reply":"2023-06-06T14:09:23.294215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that each of the training directories contain 75 images and each of the testing directories contain 250 images.\n\nLet's define our training and test filepaths.","metadata":{}},{"cell_type":"code","source":"# Create training and test directories\ntrain_dir = \"10_food_classes_10_percent/train/\"\ntest_dir = \"10_food_classes_10_percent/test/\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:23.297941Z","iopub.execute_input":"2023-06-06T14:09:23.298206Z","iopub.status.idle":"2023-06-06T14:09:23.303573Z","shell.execute_reply.started":"2023-06-06T14:09:23.298183Z","shell.execute_reply":"2023-06-06T14:09:23.302578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we've got some image data, we need a way of loading it into a TensorFlow compatible format.\n\nPreviously, we've used the [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) class. And while this works well and is still very commonly used, this time we're going to use the `image_data_from_directory` function.\n\nIt works much the same way as `ImageDataGenerator`'s `flow_from_directory` method meaning your images need to be in the following file format:\n\n```\nExample of file structure\n\n10_food_classes_10_percent <- top level folder\nâ””â”€â”€â”€train <- training images\nâ”‚   â””â”€â”€â”€pizza\nâ”‚   â”‚   â”‚   1008104.jpg\nâ”‚   â”‚   â”‚   1638227.jpg\nâ”‚   â”‚   â”‚   ...      \nâ”‚   â””â”€â”€â”€steak\nâ”‚       â”‚   1000205.jpg\nâ”‚       â”‚   1647351.jpg\nâ”‚       â”‚   ...\nâ”‚   \nâ””â”€â”€â”€test <- testing images\nâ”‚   â””â”€â”€â”€pizza\nâ”‚   â”‚   â”‚   1001116.jpg\nâ”‚   â”‚   â”‚   1507019.jpg\nâ”‚   â”‚   â”‚   ...      \nâ”‚   â””â”€â”€â”€steak\nâ”‚       â”‚   100274.jpg\nâ”‚       â”‚   1653815.jpg\nâ”‚       â”‚   ...    \n```\n\nOne of the main benefits of using [`tf.keras.prepreprocessing.image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) rather than `ImageDataGenerator` is that it creates a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) object rather than a generator. The main advantage of this is the `tf.data.Dataset` API is much more efficient (faster) than the `ImageDataGenerator` API which is paramount for larger datasets.\n\nLet's see it in action.","metadata":{}},{"cell_type":"code","source":"# Create data inputs\nimport tensorflow as tf\nIMG_SIZE = (224, 224) # define image size\ntrain_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n                                                                            image_size=IMG_SIZE,\n                                                                            label_mode=\"categorical\", # what type are the labels?\n                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\ntest_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n                                                                           image_size=IMG_SIZE,\n                                                                           label_mode=\"categorical\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:23.305147Z","iopub.execute_input":"2023-06-06T14:09:23.305554Z","iopub.status.idle":"2023-06-06T14:09:26.526304Z","shell.execute_reply.started":"2023-06-06T14:09:23.305523Z","shell.execute_reply":"2023-06-06T14:09:26.525407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wonderful! Looks like our dataloaders have found the correct number of images for each dataset.\n\nFor now, the main parameters we're concerned about in the `image_dataset_from_directory()` funtion are:\n* `directory` - the filepath of the target directory we're loading images in from.\n* `image_size` - the target size of the images we're going to load in (height, width).\n* `batch_size` - the batch size of the images we're going to load in. For example if the `batch_size` is 32 (the default), batches of 32 images and labels at a time will be passed to the model.\n\nThere are more we could play around with if we needed to [in the `tf.keras.preprocessing` documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory).\n\nIf we check the training data datatype we should see it as a `BatchDataset` with shapes relating to our data.","metadata":{}},{"cell_type":"code","source":"# Check the training data datatype\ntrain_data_10_percent","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:26.527836Z","iopub.execute_input":"2023-06-06T14:09:26.528176Z","iopub.status.idle":"2023-06-06T14:09:26.540332Z","shell.execute_reply.started":"2023-06-06T14:09:26.528144Z","shell.execute_reply":"2023-06-06T14:09:26.539377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above output:\n\n* `(None, 224, 224, 3)` refers to the tensor shape of our images where `None` is the batch size, `224` is the height (and width) and `3` is the color channels (red, green, blue).\n* `(None, 10)` refers to the tensor shape of the labels where `None` is the batch size and `10` is the number of possible labels (the 10 different food classes).\n* Both image tensors and labels are of the datatype `tf.float32`.\n\nThe `batch_size` is `None` due to it only being used during model training. You can think of `None` as a placeholder waiting to be filled with the `batch_size` parameter from `image_dataset_from_directory()`.\n\nAnother benefit of using the `tf.data.Dataset` API are the assosciated methods which come with it.\n\nFor example, if we want to find the name of the classes we were working with, we could use the `class_names` attribute.","metadata":{}},{"cell_type":"code","source":"# Check out the class names of our dataset\ntrain_data_10_percent.class_names","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:26.543555Z","iopub.execute_input":"2023-06-06T14:09:26.543828Z","iopub.status.idle":"2023-06-06T14:09:26.564840Z","shell.execute_reply.started":"2023-06-06T14:09:26.543805Z","shell.execute_reply":"2023-06-06T14:09:26.563906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Or if we wanted to see an example batch of data, we could use the `take()` method. ","metadata":{}},{"cell_type":"code","source":"# # See an example batch of data\n# for images, labels in train_data_10_percent.take(1):\n#   print(images, labels)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:26.567638Z","iopub.execute_input":"2023-06-06T14:09:26.567956Z","iopub.status.idle":"2023-06-06T14:09:26.573355Z","shell.execute_reply.started":"2023-06-06T14:09:26.567933Z","shell.execute_reply":"2023-06-06T14:09:26.572375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice how the image arrays come out as tensors of pixel values where as the labels come out as one-hot encodings (e.g. `[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]` for `hamburger`).","metadata":{}},{"cell_type":"markdown","source":"### Model 0: Building a transfer learning model using the Keras Functional API\n\nAlright, our data is tensor-ified, let's build a model.\n\nTo do so we're going to be using the [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) module as it contains a series of already trained (on ImageNet) computer vision models as well as the Keras Functional API to construct our model.\n\nWe're going to go through the following steps:\n\n1. Instantiate a pre-trained base model object by choosing a target model such as [`EfficientNetB0`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0) from `tf.keras.applications`, setting the `include_top` parameter to `False` (we do this because we're going to create our own top, which are the output layers for the model).\n2. Set the base model's `trainable` attribute to `False` to freeze all of the weights in the pre-trained model.\n3. Define an input layer for our model, for example, what shape of data should our model expect?\n4. [Optional] Normalize the inputs to our model if it requires. Some computer vision models such as `ResNetV250` require their inputs to be between 0 & 1. \n\n> ðŸ¤” **Note:** As of writing, the `EfficientNet` models in the `tf.keras.applications` module do not require images to be normalized (pixel values between 0 and 1) on input, where as many of the other models do. I posted [an issue to the TensorFlow GitHub](https://github.com/tensorflow/tensorflow/issues/42506) about this and they confirmed this. \n\n5. Pass the inputs to the base model.\n6. Pool the outputs of the base model into a shape compatible with the output activation layer (turn base model output tensors into same shape as label tensors). This can be done using [`tf.keras.layers.GlobalAveragePooling2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) or [`tf.keras.layers.GlobalMaxPooling2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool2D?hl=en) though the former is more common in practice.\n7. Create an output activation layer using `tf.keras.layers.Dense()` with the appropriate activation function and number of neurons.\n8. Combine the inputs and outputs layer into a model using [`tf.keras.Model()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n9. Compile the model using the appropriate loss function and choose of optimizer.\n10. Fit the model for desired number of epochs and with necessary callbacks (in our case, we'll start off with the TensorBoard callback).\n\nWoah... that sounds like a lot. Before we get ahead of ourselves, let's see it in practice.","metadata":{}},{"cell_type":"code","source":"# 1. Create base model with tf.keras.applications\nbase_model = tf.keras.applications.EfficientNetB0(include_top=False)\n\n# 2. Freeze the base model (so the pre-learned patterns remain)\nbase_model.trainable = False\n\n# 3. Create inputs into the base model\ninputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n\n# 4. If using ResNet50V2, add this to speed up convergence, remove for EfficientNet\n# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n\n# 5. Pass the inputs to the base_model (note: using tf.keras.applications, EfficientNet inputs don't have to be normalized)\nx = base_model(inputs)\n# Check data shape after passing it to base_model\nprint(f\"Shape after base_model: {x.shape}\")\n\n# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\nx = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\nprint(f\"After GlobalAveragePooling2D(): {x.shape}\")\n\n# 7. Create the output activation layer\noutputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n\n# 8. Combine the inputs with the outputs into a model\nmodel_0 = tf.keras.Model(inputs, outputs)\n\n# 9. Compile the model\nmodel_0.compile(loss='categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# 10. Fit the model (we use less steps for validation so it's faster)\nhistory_10_percent = model_0.fit(train_data_10_percent,\n                                 epochs=5,\n                                 steps_per_epoch=len(train_data_10_percent),\n                                 validation_data=test_data_10_percent,\n                                 # Go through less of the validation data so epochs are faster (we want faster experiments!)\n                                 validation_steps=int(0.25 * len(test_data_10_percent)), \n                                 # Track our model's training logs for visualization later\n                                 callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_feature_extract\")])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:09:26.578098Z","iopub.execute_input":"2023-06-06T14:09:26.578344Z","iopub.status.idle":"2023-06-06T14:10:07.814239Z","shell.execute_reply.started":"2023-06-06T14:09:26.578322Z","shell.execute_reply":"2023-06-06T14:10:07.813098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_0.evaluate(test_data_10_percent)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:07.817623Z","iopub.execute_input":"2023-06-06T14:10:07.817924Z","iopub.status.idle":"2023-06-06T14:10:18.076394Z","shell.execute_reply.started":"2023-06-06T14:10:07.817898Z","shell.execute_reply":"2023-06-06T14:10:18.075416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice! After a minute or so of training our model performs incredibly well on both the training (87%+ accuracy) and test sets (~83% accuracy).\n\nThis is incredible. All thanks to the power of transfer learning.\n\nIt's important to note the kind of transfer learning we used here is called feature extraction transfer learning, similar to what we did with the TensorFlow Hub models.\n\nIn other words, we passed our custom data to an already pre-trained model (`EfficientNetB0`), asked it \"what patterns do you see?\" and then put our own output layer on top to make sure the outputs were tailored to our desired number of classes. \n\nWe also used the Keras Functional API to build our model rather than the Sequential API. For now, the benefits of this main not seem clear but when you start to build more sophisticated models, you'll probably want to use the Functional API. So it's important to have exposure to this way of building models.\n\n> ðŸ“– **Resource:** To see the benefits and use cases of the Functional API versus the Sequential API, check out the [TensorFlow Functional API documentation](https://www.tensorflow.org/guide/keras/functional).\n\nLet's inspect the layers in our model, we'll start with the base.","metadata":{}},{"cell_type":"code","source":"# Check layers in our base model\nfor layer_number, layer in enumerate(base_model.layers):\n  print(layer_number, layer.name)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:18.078186Z","iopub.execute_input":"2023-06-06T14:10:18.078552Z","iopub.status.idle":"2023-06-06T14:10:18.087919Z","shell.execute_reply.started":"2023-06-06T14:10:18.078519Z","shell.execute_reply":"2023-06-06T14:10:18.086940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, that's a lot of layers... to handcode all of those would've taken a fairly long time to do, yet we can still take advatange of them thanks to the power of transfer learning.\n\nHow about a summary of the base model?","metadata":{}},{"cell_type":"code","source":"base_model.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:18.089663Z","iopub.execute_input":"2023-06-06T14:10:18.090346Z","iopub.status.idle":"2023-06-06T14:10:18.548618Z","shell.execute_reply.started":"2023-06-06T14:10:18.090313Z","shell.execute_reply":"2023-06-06T14:10:18.547841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see how each of the different layers have a certain number of parameters each. Since we are using a pre-trained model, you can think of all of these parameters are patterns the base model has learned on another dataset. And because we set `base_model.trainable = False`, these patterns remain as they are during training (they're frozen and don't get updated).\n\nAlright that was the base model, let's see the summary of our overall model.","metadata":{}},{"cell_type":"code","source":"# Check summary of model constructed with Functional API\nmodel_0.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:18.549731Z","iopub.execute_input":"2023-06-06T14:10:18.550295Z","iopub.status.idle":"2023-06-06T14:10:18.606545Z","shell.execute_reply.started":"2023-06-06T14:10:18.550255Z","shell.execute_reply":"2023-06-06T14:10:18.605774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our overall model has five layers but really, one of those layers (`efficientnetb0`) has 236 layers.\n\nYou can see how the output shape started out as `(None, 224, 224, 3)` for the input layer (the shape of our images) but was transformed to be `(None, 10)` by the output layer (the shape of our labels), where `None` is the placeholder for the batch size.\n\nNotice too, the only trainable parameters in the model are those in the output layer.\n\nHow do our model's training curves look?\n\n","metadata":{}},{"cell_type":"code","source":"# Check out our model's training curves\nplot_loss_curves(history_10_percent)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:10:18.607457Z","iopub.execute_input":"2023-06-06T14:10:18.607772Z","iopub.status.idle":"2023-06-06T14:10:19.669309Z","shell.execute_reply.started":"2023-06-06T14:10:18.607742Z","shell.execute_reply":"2023-06-06T14:10:19.668254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting a feature vector from a trained model\n\n> ðŸ¤” **Question:** What happens with the `tf.keras.layers.GlobalAveragePooling2D()` layer? I haven't seen it before.\n\nThe [`tf.keras.layers.GlobalAveragePooling2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) layer transforms a 4D tensor into a 2D tensor by averaging the values across the inner-axes.\n\nThe previous sentence is a bit of a mouthful, so let's see an example.","metadata":{"execution":{"iopub.status.busy":"2023-05-30T19:59:06.295847Z","iopub.execute_input":"2023-05-30T19:59:06.296551Z","iopub.status.idle":"2023-05-30T19:59:06.306731Z","shell.execute_reply.started":"2023-05-30T19:59:06.296517Z","shell.execute_reply":"2023-05-30T19:59:06.305002Z"}}},{"cell_type":"code","source":"# Define input tensor shape (same number of dimensions as the output of efficientnetb0)\ninput_shape = (1, 4, 4, 3)\n\n# Create a random tensor\ntf.random.set_seed(42)\ninput_tensor = tf.random.normal(input_shape)\nprint(f\"Random input tensor:\\n {input_tensor}\\n\")\n\n# Pass the random tensor through a global average pooling 2D layer\nglobal_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\nprint(f\"2D global average pooled random tensor:\\n {global_average_pooled_tensor}\\n\")\n\n# Check the shapes of the different tensors\nprint(f\"Shape of input tensor: {input_tensor.shape}\")\nprint(f\"Shape of 2D global averaged pooled input tensor: {global_average_pooled_tensor.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:10:19.670497Z","iopub.execute_input":"2023-06-06T14:10:19.670791Z","iopub.status.idle":"2023-06-06T14:10:19.724697Z","shell.execute_reply.started":"2023-06-06T14:10:19.670767Z","shell.execute_reply":"2023-06-06T14:10:19.723663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see the `tf.keras.layers.GlobalAveragePooling2D()` layer condensed the input tensor from shape `(1, 4, 4, 3)` to `(1, 3)`. It did so by averaging the `input_tensor` across the middle two axes.\n\nWe can replicate this operation using the `tf.reduce_mean()` operation and specifying the appropriate axes.","metadata":{}},{"cell_type":"code","source":"# This is the same as GlobalAveragePooling2D()\ntf.reduce_mean(input_tensor, axis=[1, 2]) # average across the middle axes","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:19.726466Z","iopub.execute_input":"2023-06-06T14:10:19.726900Z","iopub.status.idle":"2023-06-06T14:10:19.735175Z","shell.execute_reply.started":"2023-06-06T14:10:19.726865Z","shell.execute_reply":"2023-06-06T14:10:19.734006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Doing this not only makes the output of the base model compatible with the input shape requirement of our output layer (`tf.keras.layers.Dense()`), it also condenses the information found by the base model into a lower dimension **feature vector**.\n\n> ðŸ”‘ **Note:** One of the reasons feature extraction transfer learning is named how it is is because what often happens is a pretrained model outputs a **feature vector** (a long tensor of numbers, in our case, this is the output of the [`tf.keras.layers.GlobalAveragePooling2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) layer) which can then be used to extract patterns out of.\n\n> ðŸ›  **Practice:** Do the same as the above cell but for [`tf.keras.layers.GlobalMaxPool2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool2D).1","metadata":{}},{"cell_type":"markdown","source":"## Running a series of transfer learning experiments\n\nWe've seen the incredible results of transfer learning on 10% of the training data, what about 1% of the training data?\n\nWhat kind of results do you think we can get using 100x less data than the original CNN models we built ourselves?\n\nWhy don't we answer that question while running the following modelling experiments:\n1. `model_1`: Use feature extraction transfer learning on 1% of the training data with data augmentation.\n2. `model_2`: Use feature extraction transfer learning on 10% of the training data with data augmentation.\n3. `model_3`: Use fine-tuning transfer learning on 10% of the training data with data augmentation.\n4. `model_4`: Use fine-tuning transfer learning on 100% of the training data with data augmentation.\n\nWhile all of the experiments will be run on different versions of the training data, they will all be evaluated on the same test dataset, this ensures the results of each experiment are as comparable as possible.\n\nAll experiments will be done using the `EfficientNetB0` model within the `tf.keras.applications` module.\n\nTo make sure we're keeping track of our experiments, we'll use our `create_tensorboard_callback()` function to log all of the model training logs.\n\nWe'll construct each model using the Keras Functional API and instead of implementing data augmentation in the `ImageDataGenerator` class as we have previously, we're going to build it right into the model using the [`tf.keras.layers.experimental.preprocessing`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) module.\n\nLet's begin by downloading the data for experiment 1, using feature extraction transfer learning on 1% of the training data with data augmentation.","metadata":{}},{"cell_type":"code","source":"# Download and unzip data\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\nunzip_data(\"10_food_classes_1_percent.zip\")\n\n# Create training and test dirs\ntrain_dir_1_percent = \"10_food_classes_1_percent/train/\"\ntest_dir = \"10_food_classes_1_percent/test/\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:19.736818Z","iopub.execute_input":"2023-06-06T14:10:19.737781Z","iopub.status.idle":"2023-06-06T14:10:22.597498Z","shell.execute_reply.started":"2023-06-06T14:10:19.737742Z","shell.execute_reply":"2023-06-06T14:10:22.596393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How many images are we working with?","metadata":{}},{"cell_type":"code","source":"# Walk through 1 percent data directory and list number of files\nwalk_through_dir(\"10_food_classes_1_percent\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:22.599572Z","iopub.execute_input":"2023-06-06T14:10:22.600140Z","iopub.status.idle":"2023-06-06T14:10:22.611106Z","shell.execute_reply.started":"2023-06-06T14:10:22.600102Z","shell.execute_reply":"2023-06-06T14:10:22.610114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, looks like we've only got seven images of each class, this should be a bit of a challenge for our model.\n\n> ðŸ”‘ **Note:** As with the 10% of data subset, the 1% of images were chosen at random from the original full training dataset. The test images are the same as the ones which have previously been used. If you want to see how this data was preprocessed, check out the [Food Vision Image Preprocessing notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb).\n\nTime to load our images in as `tf.data.Dataset` objects, to do so, we'll use the [`image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) method. ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nIMG_SIZE = (224, 224)\ntrain_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,\n                                                                           label_mode=\"categorical\",\n                                                                           batch_size=32, # default\n                                                                           image_size=IMG_SIZE)\ntest_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n                                                                label_mode=\"categorical\",\n                                                                image_size=IMG_SIZE)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:22.612821Z","iopub.execute_input":"2023-06-06T14:10:22.613405Z","iopub.status.idle":"2023-06-06T14:10:22.918554Z","shell.execute_reply.started":"2023-06-06T14:10:22.613375Z","shell.execute_reply":"2023-06-06T14:10:22.917646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data loaded. Time to augment it.","metadata":{}},{"cell_type":"markdown","source":"### Adding data augmentation right into the model\n\nPreviously we've used the different parameters of the `ImageDataGenerator` class to augment our training images, this time we're going to build data augmentation right into the model.\n\nHow?\n\nUsing the [`tf.keras.layers.experimental.preprocessing`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) module and creating a dedicated data augmentation layer.\n\nThis a relatively new feature added to TensorFlow 2.2+ but it's very powerful. Adding a data augmentation layer to the model has the following benefits:\n* Preprocessing of the images (augmenting them) happens on the GPU rather than on the CPU (much faster).\n  * Images are best preprocessed on the GPU where as text and structured data are more suited to be preprocessed on the CPU.\n* Image data augmentation only happens during training so we can still export our whole model and use it elsewhere. And if someone else wanted to train the same model as us, including the same kind of data augmentation, they could.\n\n![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-data-augmentation-inside-a-model.png)\n*Example of using data augmentation as the first layer within a model (EfficientNetB0).*\n\n> ðŸ¤” **Note:** At the time of writing, the preprocessing layers we're using for data augmentation are in *experimental* status within the in TensorFlow library. This means although the layers should be considered stable, the code may change slightly in a future version of TensorFlow. For more information on the other preprocessing layers avaiable and the different methods of data augmentation, check out the [Keras preprocessing layers guide](https://keras.io/guides/preprocessing_layers/) and the [TensorFlow data augmentation guide](https://www.tensorflow.org/tutorials/images/data_augmentation).\n\nTo use data augmentation right within our model we'll create a Keras Sequential model consisting of only data preprocessing layers, we can then use this Sequential model within another Functional model.\n\nIf that sounds confusing, it'll make sense once we create it in code.\n\nThe data augmentation transformations we're going to use are:\n* [RandomFlip](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomFlip) - flips image on horizontal or vertical axis.\n* [RandomRotation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomRotation) - randomly rotates image by a specified amount.\n* [RandomZoom](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomZoom) - randomly zooms into an image by specified amount.\n* [RandomHeight](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomHeight) - randomly shifts image height by a specified amount.\n* [RandomWidth](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomWidth) - randomly shifts image width by a specified amount.\n* [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling) - normalizes the image pixel values to be between 0 and 1, this is worth mentioning because it is required for some image models but since we're using the `tf.keras.applications` implementation of `EfficientNetB0`, it's not required.\n\nThere are more option but these will do for now.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation = keras.Sequential([\n  preprocessing.RandomFlip(\"horizontal\"),\n  preprocessing.RandomRotation(0.2),\n  preprocessing.RandomZoom(0.2),\n  preprocessing.RandomHeight(0.2),\n  preprocessing.RandomWidth(0.2),\n  #preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetB0\n], name =\"data_augmentation\")\n\n# resize_and_rescaling=keras.Sequential([\n#     layers.resize(224,224),\n#     layers.rescaling(1./255)\n# ])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:22.919897Z","iopub.execute_input":"2023-06-06T14:10:22.920324Z","iopub.status.idle":"2023-06-06T14:10:22.944972Z","shell.execute_reply.started":"2023-06-06T14:10:22.920290Z","shell.execute_reply":"2023-06-06T14:10:22.944130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And that's it! Our data augmentation Sequential model is ready to go. As you'll see shortly, we'll be able to slot this \"model\" as a layer into our transfer learning model later on.\n\nBut before we do that, let's test it out by passing random images through it.","metadata":{"execution":{"iopub.status.busy":"2023-05-31T17:50:09.969128Z","iopub.execute_input":"2023-05-31T17:50:09.969889Z","iopub.status.idle":"2023-05-31T17:50:09.976786Z","shell.execute_reply.started":"2023-05-31T17:50:09.969853Z","shell.execute_reply":"2023-05-31T17:50:09.975783Z"}}},{"cell_type":"code","source":"# View a random image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os\nimport random\ntarget_class = random.choice(train_data_1_percent.class_names) # choose a random class\ntarget_dir = \"10_food_classes_1_percent/train/\" + target_class # create the target directory\nrandom_image = random.choice(os.listdir(target_dir)) # choose a random image from target directory\nrandom_image_path = target_dir + \"/\" + random_image # create the choosen random image path\nimg = mpimg.imread(random_image_path) # read in the chosen target image\nplt.imshow(img) # plot the target image\nplt.title(f\"Original random image from class: {target_class}\")\nplt.axis(False); # turn off the axes\n\n# Augment the image\naugmented_img = data_augmentation(tf.expand_dims(img, axis=0),training=True) # data augmentation model requires shape (None, height, width, 3)\nplt.figure()\nplt.imshow(tf.squeeze(augmented_img)/255.) # requires normalization after augmentation\nplt.title(f\"Augmented random image from class: {target_class}\")\nplt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:10:22.946582Z","iopub.execute_input":"2023-06-06T14:10:22.946933Z","iopub.status.idle":"2023-06-06T14:10:23.949895Z","shell.execute_reply.started":"2023-06-06T14:10:22.946901Z","shell.execute_reply":"2023-06-06T14:10:23.948885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the cell above a few times and you can see the different random augmentations on different classes of images. Because we're going to add the data augmentation model as a layer in our upcoming transfer learning model, it'll apply these kind of random augmentations to each of the training images which passes through it. \n\nDoing this will make our training dataset a little more varied. You can think of it as if you were taking a photo of food in real-life, not all of the images are going to be perfect, some of them are going to be orientated in strange ways. These are the kind of images we want our model to be able to handle.\n\nSpeaking of model, let's build one with the Functional API. We'll run through all of the same steps as before except for one difference, we'll add our data augmentation Sequential model as a layer immediately after the input layer.","metadata":{}},{"cell_type":"markdown","source":"## Model 1: Feature extraction transfer learning on 1% of the data with data augmentation","metadata":{}},{"cell_type":"code","source":"# Setup input shape and base model, freezing the base model layers\ninput_shape = (224, 224, 3)\nbase_model = tf.keras.applications.EfficientNetB0(include_top=False)\nbase_model.trainable = False\n\n# Create input layer\ninputs = layers.Input(shape=input_shape, name=\"input_layer\")\n\n# Add in data augmentation Sequential model as a layer\nx = data_augmentation(inputs)\n\n# Give base_model inputs (after augmentation) and don't train it\nx = base_model(x, training=False)\n\n# Pool output features of base model\nx = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n\n# Put a dense layer on as the output\noutputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n\n# Make a model with inputs and outputs\nmodel_1 = keras.Model(inputs, outputs)\n\n# Compile the model\nmodel_1.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_1_percent = model_1.fit(train_data_1_percent,\n                    epochs=5,\n                    steps_per_epoch=len(train_data_1_percent),\n                    validation_data=test_data,\n                    validation_steps=int(0.25* len(test_data)), # validate for less steps\n                    # Track model training logs\n                    callbacks=[create_tensorboard_callback(\"transfer_learning\", \"1_percent_data_aug\")])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:10:23.951640Z","iopub.execute_input":"2023-06-06T14:10:23.952017Z","iopub.status.idle":"2023-06-06T14:11:02.606918Z","shell.execute_reply.started":"2023-06-06T14:10:23.951981Z","shell.execute_reply":"2023-06-06T14:11:02.605995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! How cool is that? Using only 7 training images per class, using transfer learning our model was able to get ~40% accuracy on the validation set. This result is pretty amazing since the [original Food-101 paper](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf) achieved 50.67% accuracy with all the data, namely, 750 training images per class (**note:** this metric was across 101 classes, not 10, we'll get to 101 classes soon).\n\nIf we check out a summary of our model, we should see the data augmentation layer just after the input layer.","metadata":{}},{"cell_type":"code","source":"# Check out model summary\nmodel_1.summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:11:02.608323Z","iopub.execute_input":"2023-06-06T14:11:02.609050Z","iopub.status.idle":"2023-06-06T14:11:02.650874Z","shell.execute_reply.started":"2023-06-06T14:11:02.609000Z","shell.execute_reply":"2023-06-06T14:11:02.649988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There it is. We've now got data augmentation built right into the our model. This means if we saved it and reloaded it somewhere else, the data augmentation layers would come with it.\n\nThe important thing to remember is **data augmentation only runs during training**. So if we were to evaluate or use our model for inference (predicting the class of an image) the data augmentation layers will be automatically turned off.\n\nTo see this in action, let's evaluate our model on the test data.","metadata":{}},{"cell_type":"code","source":"# Evaluate on the test data\nresults_1_percent_data_aug = model_1.evaluate(test_data)\nresults_1_percent_data_aug","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:11:02.654389Z","iopub.execute_input":"2023-06-06T14:11:02.654754Z","iopub.status.idle":"2023-06-06T14:11:07.843495Z","shell.execute_reply.started":"2023-06-06T14:11:02.654719Z","shell.execute_reply":"2023-06-06T14:11:07.842642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results here may be slightly better/worse than the log outputs of our model during training because during training we only evaluate our model on 25% of the test data using the line `validation_steps=int(0.25 * len(test_data))`. Doing this speeds up our epochs but still gives us enough of an idea of how our model is going.\n\nLet's stay consistent and check out our model's loss curves.","metadata":{"execution":{"iopub.status.busy":"2023-05-31T20:25:36.654266Z","iopub.execute_input":"2023-05-31T20:25:36.654597Z","iopub.status.idle":"2023-05-31T20:25:36.665285Z","shell.execute_reply.started":"2023-05-31T20:25:36.654569Z","shell.execute_reply":"2023-05-31T20:25:36.663677Z"}}},{"cell_type":"code","source":"# How does the model go with a data augmentation layer with 1% of data\nplot_loss_curves(history_1_percent)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:11:07.844860Z","iopub.execute_input":"2023-06-06T14:11:07.845254Z","iopub.status.idle":"2023-06-06T14:11:08.556499Z","shell.execute_reply.started":"2023-06-06T14:11:07.845216Z","shell.execute_reply":"2023-06-06T14:11:08.555559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like the metrics on both datasets would improve if we kept training for more epochs. But we'll leave that for now, we've got more experiments to do!","metadata":{}},{"cell_type":"markdown","source":"## Model 2: Feature extraction transfer learning with 10% of data and data augmentation\n\nAlright, we've tested 1% of the training data with data augmentation, how about we try 10% of the data with data augmentation?\n\nBut wait...\n\n> ðŸ¤” **Question:** How do you know what experiments to run?\n\nGreat question. \n\nThe truth here is you often won't. Machine learning is still a very experimental practice. It's only after trying a fair few things that you'll start to develop an intuition of what to try.\n\nMy advice is to follow your curiosity as tenaciously as possible. If you feel like you want to try something, write the code for it and run it. See how it goes. The worst thing that'll happen is you'll figure out what doesn't work, the most valuable kind of knowledge.\n\nFrom a practical standpoint, as we've talked about before, you'll want to reduce the amount of time between your initial experiments as much as possible. In other words, run a plethora of smaller experiments, using less data and less training iterations before you find something promising and then scale it up.\n\nIn the theme of scale, let's scale our 1% training data augmentation experiment up to 10% training data augmentation. That sentence doesn't really make sense but you get what I mean.\n\nWe're going to run through the exact same steps as the previous model, the only difference being using 10% of the training data instead of 1%.","metadata":{}},{"cell_type":"code","source":"# Get 10% of the data of the 10 classes (uncomment if you haven't gotten \"10_food_classes_10_percent.zip\" already)\n# !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n# unzip_data(\"10_food_classes_10_percent.zip\")\n\ntrain_dir_10_percent = \"10_food_classes_10_percent/train/\"\ntest_dir = \"10_food_classes_10_percent/test/\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:11:08.558012Z","iopub.execute_input":"2023-06-06T14:11:08.558365Z","iopub.status.idle":"2023-06-06T14:11:08.563112Z","shell.execute_reply.started":"2023-06-06T14:11:08.558332Z","shell.execute_reply":"2023-06-06T14:11:08.562154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data downloaded. Let's create the dataloaders.","metadata":{"execution":{"iopub.status.busy":"2023-05-31T20:26:16.851828Z","iopub.execute_input":"2023-05-31T20:26:16.852748Z","iopub.status.idle":"2023-05-31T20:26:16.859133Z","shell.execute_reply.started":"2023-05-31T20:26:16.852715Z","shell.execute_reply":"2023-05-31T20:26:16.857837Z"}}},{"cell_type":"code","source":"# Setup data inputs\nimport tensorflow as tf\nIMG_SIZE = (224, 224)\ntrain_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,\n                                                                            label_mode=\"categorical\",\n                                                                            image_size=IMG_SIZE)\n# Note: the test data is the same as the previous experiment, we could\n# skip creating this, but we'll leave this here to practice.\ntest_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n                                                                label_mode=\"categorical\",\n                                                                image_size=IMG_SIZE)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:11:08.571505Z","iopub.execute_input":"2023-06-06T14:11:08.571829Z","iopub.status.idle":"2023-06-06T14:11:08.934688Z","shell.execute_reply.started":"2023-06-06T14:11:08.571802Z","shell.execute_reply":"2023-06-06T14:11:08.933773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a functional model with data augmentation\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.models import Sequential\n\n# Build data augmentation layer\ndata_augmentation = Sequential([\n  preprocessing.RandomFlip('horizontal'),\n  preprocessing.RandomHeight(0.2),\n  preprocessing.RandomWidth(0.2),\n  preprocessing.RandomZoom(0.2),\n  preprocessing.RandomRotation(0.2),\n  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNet                 \n], name=\"data_augmentation\")\n\n# Setup the input shape to our model\ninput_shape = (224, 224, 3)\n\n# Create a frozen base model\nbase_model = tf.keras.applications.EfficientNetB0(include_top=False)\nbase_model.trainable = False\n\n# Create input and output layers\ninputs = layers.Input(shape=input_shape, name=\"input_layer\") # create input layer\nx = data_augmentation(inputs) # augment our training images\nx = base_model(x, training=False) # pass augmented images to base model but keep it in inference mode, so batchnorm layers don't get updated: https://keras.io/guides/transfer_learning/#build-a-model \nx = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\noutputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\nmodel_2 = tf.keras.Model(inputs, outputs)\n\n# Compile\nmodel_2.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(lr=0.001), # use Adam optimizer with base learning rate\n              metrics=[\"accuracy\"])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:11:08.936688Z","iopub.execute_input":"2023-06-06T14:11:08.937035Z","iopub.status.idle":"2023-06-06T14:11:12.556832Z","shell.execute_reply.started":"2023-06-06T14:11:08.937002Z","shell.execute_reply":"2023-06-06T14:11:12.555895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:11:12.558265Z","iopub.execute_input":"2023-06-06T14:11:12.558619Z","iopub.status.idle":"2023-06-06T14:11:12.600013Z","shell.execute_reply.started":"2023-06-06T14:11:12.558570Z","shell.execute_reply":"2023-06-06T14:11:12.599040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating a ModelCheckpoint callback\n\nOur model is compiled and ready to be fit, so why haven't we fit it yet?\n\nWell, for this experiment we're going to introduce a new callback, the `ModelCheckpoint` callback.\n\nThe [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback gives you the ability to save your model, as a whole in the [`SavedModel`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) format or the [weights (patterns) only](https://www.tensorflow.org/tutorials/keras/save_and_load#manually_save_weights) to a specified directory as it trains. \n\nThis is helpful if you think your model is going to be training for a long time and you want to make backups of it as it trains. It also means if you think your model could benefit from being trained for longer, you can reload it from a specific checkpoint and continue training from there.\n\nFor example, say you fit a feature extraction transfer learning model for 5 epochs and you check the training curves and see it was still improving and you want to see if fine-tuning for another 5 epochs could help, you can load the checkpoint, unfreeze some (or all) of the base model layers and then continue training.\n\nIn fact, that's exactly what we're going to do. \n\nBut first, let's create a `ModelCheckpoint` callback. To do so, we have to specifcy a directory we'd like to save to.","metadata":{}},{"cell_type":"code","source":"# Setup checkpoint path\ncheckpoint_path = \"ten_percent_model_checkpoints_weights/checkpoint.ckpt\" # note: remember saving directly to Colab is temporary\n\n# Create a ModelCheckpoint callback that saves the model's weights only\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                         save_weights_only=True, # set to False to save the entire model\n                                                         save_best_only=False, # set to True to save only the best model instead of a model every epoch \n                                                         save_freq=\"epoch\", # save every epoch\n                                                         verbose=1)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:11:12.601440Z","iopub.execute_input":"2023-06-06T14:11:12.602018Z","iopub.status.idle":"2023-06-06T14:11:12.607256Z","shell.execute_reply.started":"2023-06-06T14:11:12.601985Z","shell.execute_reply":"2023-06-06T14:11:12.606351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ðŸ¤” **Question:** What's the difference between saving the entire model (SavedModel format) and saving the weights only?\n\nThe [`SavedModel`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) format saves a model's architecture, weights and training configuration all in one folder. It makes it very easy to reload your model exactly how it is elsewhere. However, if you do not want to share all of these details with others, you may want to save and share the weights only (these will just be large tensors of non-human interpretable numbers). If disk space is an issue, saving the weights only is faster and takes up less space than saving the whole model.\n\nTime to fit the model.\n\nBecause we're going to be fine-tuning it later, we'll create a variable `initial_epochs` and set it to 5 to use later.\n\nWe'll also add in our `checkpoint_callback` in our list of `callbacks`.","metadata":{}},{"cell_type":"code","source":"# Fit the model saving checkpoints every epoch\ninitial_epochs = 5\nhistory_10_percent_data_aug = model_2.fit(train_data_10_percent,\n                                          epochs=initial_epochs,\n                                          validation_data=test_data,\n                                          validation_steps=int(0.25 * len(test_data)), # do less steps per validation (quicker)\n                                          callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_data_aug\"), \n                                                     checkpoint_callback])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:11:12.608611Z","iopub.execute_input":"2023-06-06T14:11:12.609166Z","iopub.status.idle":"2023-06-06T14:12:16.793268Z","shell.execute_reply.started":"2023-06-06T14:11:12.609135Z","shell.execute_reply":"2023-06-06T14:12:16.792191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the test data\nresults_10_percent_data_aug = model_2.evaluate(test_data)\nresults_10_percent_data_aug","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:12:16.795169Z","iopub.execute_input":"2023-06-06T14:12:16.795589Z","iopub.status.idle":"2023-06-06T14:12:27.042916Z","shell.execute_reply.started":"2023-06-06T14:12:16.795548Z","shell.execute_reply":"2023-06-06T14:12:27.041837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model loss curves\nplot_loss_curves(history_10_percent_data_aug)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:27.044796Z","iopub.execute_input":"2023-06-06T14:12:27.045164Z","iopub.status.idle":"2023-06-06T14:12:27.742334Z","shell.execute_reply.started":"2023-06-06T14:12:27.045130Z","shell.execute_reply":"2023-06-06T14:12:27.741283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at these, our model's performance with 10% of the data and data augmentation isn't as good as the model with 10% of the data without data augmentation (see `model_0` results above), however the curves are trending in the right direction, meaning if we decided to train for longer, its metrics would likely improve.\n\nSince we checkpointed (is that a word?) our model's weights, we might as well see what it's like to load it back in. We'll be able to test if it saved correctly by evaluting it on the test data.\n\nTo load saved model weights you can use the the [`load_weights()`](https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_options) method, passing it the path where your saved weights are stored.","metadata":{}},{"cell_type":"code","source":"# Load in saved model weights and evaluate model\nmodel_2.load_weights(checkpoint_path)\nloaded_weights_model_results = model_2.evaluate(test_data)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:12:27.743729Z","iopub.execute_input":"2023-06-06T14:12:27.744165Z","iopub.status.idle":"2023-06-06T14:12:39.378907Z","shell.execute_reply.started":"2023-06-06T14:12:27.744131Z","shell.execute_reply":"2023-06-06T14:12:39.377904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's compare the results of our previously trained model and the loaded model. These results should very close if not exactly the same. The reason for minor differences comes down to the precision level of numbers calculated.","metadata":{}},{"cell_type":"code","source":"# If the results from our native model and the loaded weights are the same, this should output True\nresults_10_percent_data_aug == loaded_weights_model_results","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:12:39.380181Z","iopub.execute_input":"2023-06-06T14:12:39.380466Z","iopub.status.idle":"2023-06-06T14:12:39.387903Z","shell.execute_reply.started":"2023-06-06T14:12:39.380442Z","shell.execute_reply":"2023-06-06T14:12:39.386786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the above cell doesn't output `True`, it's because the numbers are close but not the *exact* same (due to how computers store numbers with degrees of precision).\n\nHowever, they should be *very* close...","metadata":{}},{"cell_type":"code","source":"import numpy as np\n# Check to see if loaded model results are very close to native model results (should output True)\nnp.isclose(np.array(results_10_percent_data_aug), np.array(loaded_weights_model_results))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:12:39.389446Z","iopub.execute_input":"2023-06-06T14:12:39.390247Z","iopub.status.idle":"2023-06-06T14:12:39.400914Z","shell.execute_reply.started":"2023-06-06T14:12:39.390214Z","shell.execute_reply":"2023-06-06T14:12:39.399978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the difference between the two results\nprint(np.array(results_10_percent_data_aug) - np.array(loaded_weights_model_results))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:12:39.402316Z","iopub.execute_input":"2023-06-06T14:12:39.402900Z","iopub.status.idle":"2023-06-06T14:12:39.414759Z","shell.execute_reply.started":"2023-06-06T14:12:39.402873Z","shell.execute_reply":"2023-06-06T14:12:39.413574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 3: Fine-tuning an existing model on 10% of the data\n\n![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-fine-tuning-an-efficientnet-model.png)\n*High-level example of fine-tuning an EfficientNet model. Bottom layers (layers closer to the input data) stay frozen where as top layers (layers closer to the output data) are updated during training.*\n\nSo far our saved model has been trained using feature extraction transfer learning for 5 epochs on 10% of the training data and data augmentation.\n\nThis means all of the layers in the base model (EfficientNetB0) were frozen during training.\n\nFor our next experiment we're going to switch to fine-tuning transfer learning. This means we'll be using the same base model except we'll be unfreezing some of its layers (ones closest to the top) and running the model for a few more epochs.\n\nThe idea with fine-tuning is to start customizing the pre-trained model more to our own data.\n\n> ðŸ”‘ **Note:** Fine-tuning usually works best *after* training a feature extraction model for a few epochs and with large amounts of data. For more on this, check out [Keras' guide on Transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).\n\nWe've verified our loaded model's performance, let's check out its layers.","metadata":{}},{"cell_type":"code","source":"# Layers in loaded model\nmodel_2.layers","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:12:39.416627Z","iopub.execute_input":"2023-06-06T14:12:39.417112Z","iopub.status.idle":"2023-06-06T14:12:39.427843Z","shell.execute_reply.started":"2023-06-06T14:12:39.417074Z","shell.execute_reply":"2023-06-06T14:12:39.426548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in model_2.layers:\n  print(layer.trainable)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:12:39.429425Z","iopub.execute_input":"2023-06-06T14:12:39.429892Z","iopub.status.idle":"2023-06-06T14:12:39.437680Z","shell.execute_reply.started":"2023-06-06T14:12:39.429861Z","shell.execute_reply":"2023-06-06T14:12:39.436002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:39.439206Z","iopub.execute_input":"2023-06-06T14:12:39.440280Z","iopub.status.idle":"2023-06-06T14:12:39.485636Z","shell.execute_reply.started":"2023-06-06T14:12:39.440233Z","shell.execute_reply":"2023-06-06T14:12:39.484756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, it looks like all of the layers in the `efficientnetb0` layer are frozen. We can confirm this using the `trainable_variables` attribute.","metadata":{}},{"cell_type":"code","source":"# How many layers are trainable in our base model?\nprint(len(model_2.layers[2].trainable_variables)) # layer at index 2 is the EfficientNetB0 layer (the base model)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:39.487001Z","iopub.execute_input":"2023-06-06T14:12:39.487641Z","iopub.status.idle":"2023-06-06T14:12:39.492845Z","shell.execute_reply.started":"2023-06-06T14:12:39.487605Z","shell.execute_reply":"2023-06-06T14:12:39.491890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(base_model.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:39.494483Z","iopub.execute_input":"2023-06-06T14:12:39.495128Z","iopub.status.idle":"2023-06-06T14:12:39.503411Z","shell.execute_reply.started":"2023-06-06T14:12:39.495093Z","shell.execute_reply":"2023-06-06T14:12:39.502326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check which layers are tuneable (trainable)\nfor layer_number, layer in enumerate(base_model.layers):\n  print(layer_number, layer.name, layer.trainable)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:12:39.504865Z","iopub.execute_input":"2023-06-06T14:12:39.505576Z","iopub.status.idle":"2023-06-06T14:12:39.518805Z","shell.execute_reply.started":"2023-06-06T14:12:39.505534Z","shell.execute_reply":"2023-06-06T14:12:39.517765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Beautiful. This is exactly what we're after. \n\nNow to fine-tune the base model to our own data, we're going to unfreeze the top 10 layers and continue training our model for another 5 epochs.\n\nThis means all of the base model's layers except for the last 10 will remain frozen and untrainable. And the weights in the remaining unfrozen layers will be updated during training.\n\nIdeally, we should see the model's performance improve.\n\n> ðŸ¤” **Question:** How many layers should you unfreeze when training?\n\nThere's no set rule for this. You could unfreeze every layer in the pretrained model or you could try unfreezing one layer at a time. Best to experiment with different amounts of unfreezing and fine-tuning to see what happens. Generally, the less data you have, the less layers you want to unfreeze and the more gradually you want to fine-tune.\n\n> ðŸ“– **Resource:** The [ULMFiT (Universal Language Model Fine-tuning for Text Classification) paper](https://arxiv.org/abs/1801.06146) has a great series of experiments on fine-tuning models.\n\nTo begin fine-tuning, we'll unfreeze the entire base model by setting its `trainable` attribute to `True`. Then we'll refreeze every layer in the base model except for the last 10 by looping through them and setting their `trainable` attribute to `False`. Finally, we'll recompile the model.","metadata":{}},{"cell_type":"code","source":"base_model.trainable = True\n\n# Freeze all layers except for the last 10\nfor layer in base_model.layers[:-10]:\n  layer.trainable = False\n\n# Recompile the model (always recompile after any adjustments to a model)\nmodel_2.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(lr=0.0001), # lr is 10x lower than before for fine-tuning IMPORTANT TO AVOID OVERTRAINING\n              metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:39.520438Z","iopub.execute_input":"2023-06-06T14:12:39.521085Z","iopub.status.idle":"2023-06-06T14:12:39.556185Z","shell.execute_reply.started":"2023-06-06T14:12:39.521053Z","shell.execute_reply":"2023-06-06T14:12:39.555371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check which layers are tuneable (trainable)\nfor layer_number, layer in enumerate(base_model.layers):\n  print(layer_number, layer.name, layer.trainable)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-06T14:12:39.558454Z","iopub.execute_input":"2023-06-06T14:12:39.559010Z","iopub.status.idle":"2023-06-06T14:12:39.569355Z","shell.execute_reply.started":"2023-06-06T14:12:39.558978Z","shell.execute_reply":"2023-06-06T14:12:39.568300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice! It seems all layers except for the last 10 are frozen and untrainable. This means only the last 10 layers of the base model along with the output layer will have their weights updated during training.\n\n> ðŸ¤” **Question:** Why did we recompile the model?\n\nEvery time you make a change to your models, you need to recompile them.\n\nIn our case, we're using the exact same loss, optimizer and metrics as before, except this time the learning rate for our optimizer will be 10x smaller than before (0.0001 instead of Adam's default of 0.001).\n\nWe do this so the model doesn't try to overwrite the existing weights in the pretrained model too fast. In other words, we want learning to be more gradual.\n\n> ðŸ”‘ **Note:** There's no set standard for setting the learning rate during fine-tuning, though reductions of [2.6x-10x+ seem to work well in practice](https://arxiv.org/abs/1801.06146).\n\nHow many trainable variables do we have now?","metadata":{}},{"cell_type":"code","source":"print(len(model_2.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:39.571028Z","iopub.execute_input":"2023-06-06T14:12:39.571522Z","iopub.status.idle":"2023-06-06T14:12:39.580748Z","shell.execute_reply.started":"2023-06-06T14:12:39.571491Z","shell.execute_reply":"2023-06-06T14:12:39.579624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is where saving the history variables of our model training comes in handy. Let's see what happened after fine-tuning the last 10 layers of our model.Wonderful, it looks like our model has a total of 10 trainable variables, the last 10 layers of the base model and the weight and bias parameters of the Dense output layer.\n\nTime to fine-tune!\n\nWe're going to continue training on from where our previous model finished. Since it trained for 5 epochs, our fine-tuning will begin on the epoch 5 and continue for another 5 epochs.\n\nTo do this, we can use the `initial_epoch` parameter of the [`fit()`](https://keras.rstudio.com/reference/fit.html) method. We'll pass it the last epoch of the previous model's training history (`history_10_percent_data_aug.epoch[-1]`).","metadata":{}},{"cell_type":"code","source":"# Fine tune for another 5 epochs\nfine_tune_epochs = initial_epochs + 5\n\n# Refit the model (same as model_2 except with more trainable layers)\nhistory_fine_10_percent_data_aug = model_2.fit(train_data_10_percent,\n                                               epochs=fine_tune_epochs,\n                                               validation_data=test_data,\n                                               initial_epoch=history_10_percent_data_aug.epoch[-1], # start from previous last epoch\n                                               validation_steps=int(0.25 * len(test_data)),\n                                               callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_fine_tune_last_10\")]) # name experiment appropriately","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:12:39.582208Z","iopub.execute_input":"2023-06-06T14:12:39.582578Z","iopub.status.idle":"2023-06-06T14:13:51.857711Z","shell.execute_reply.started":"2023-06-06T14:12:39.582539Z","shell.execute_reply":"2023-06-06T14:13:51.856704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test data\nresults_fine_tune_10_percent = model_2.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:13:51.859589Z","iopub.execute_input":"2023-06-06T14:13:51.859993Z","iopub.status.idle":"2023-06-06T14:13:57.042779Z","shell.execute_reply.started":"2023-06-06T14:13:51.859958Z","shell.execute_reply":"2023-06-06T14:13:57.041754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remember, the results from evaluating the model might be slightly different to the outputs from training since during training we only evaluate on 25% of the test data.\n\nAlright, we need a way to evaluate our model's performance before and after fine-tuning. How about we write a function to compare the before and after?","metadata":{}},{"cell_type":"code","source":"def compare_historys(original_history, new_history, initial_epochs=5):\n    \"\"\"\n    Compares two model history objects.\n    \"\"\"\n    # Get original history measurements\n    acc = original_history.history[\"accuracy\"]\n    loss = original_history.history[\"loss\"]\n\n    print(len(acc))\n\n    val_acc = original_history.history[\"val_accuracy\"]\n    val_loss = original_history.history[\"val_loss\"]\n\n    # Combine original history with new history\n    total_acc = acc + new_history.history[\"accuracy\"]\n    total_loss = loss + new_history.history[\"loss\"]\n\n    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n\n    print(len(total_acc))\n    print(total_acc)\n\n    # Make plots\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(total_acc, label='Training Accuracy')\n    plt.plot(total_val_acc, label='Validation Accuracy')\n    plt.plot([initial_epochs-1, initial_epochs-1],\n              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(total_loss, label='Training Loss')\n    plt.plot(total_val_loss, label='Validation Loss')\n    plt.plot([initial_epochs-1, initial_epochs-1],\n              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:13:57.047370Z","iopub.execute_input":"2023-06-06T14:13:57.049656Z","iopub.status.idle":"2023-06-06T14:13:57.070133Z","shell.execute_reply.started":"2023-06-06T14:13:57.049618Z","shell.execute_reply":"2023-06-06T14:13:57.068807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is where saving the history variables of our model training comes in handy. Let's see what happened after fine-tuning the last 10 layers of our model.","metadata":{}},{"cell_type":"code","source":"compare_historys(original_history=history_10_percent_data_aug, \n                 new_history=history_fine_10_percent_data_aug, \n                 initial_epochs=5)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:13:57.078054Z","iopub.execute_input":"2023-06-06T14:13:57.078937Z","iopub.status.idle":"2023-06-06T14:13:57.615520Z","shell.execute_reply.started":"2023-06-06T14:13:57.078744Z","shell.execute_reply":"2023-06-06T14:13:57.614573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, alright, seems like the curves are heading in the right direction after fine-tuning. But remember, it should be noted that fine-tuning usually works best with larger amounts of data.","metadata":{"execution":{"iopub.status.busy":"2023-06-03T13:31:41.388546Z","iopub.execute_input":"2023-06-03T13:31:41.388945Z","iopub.status.idle":"2023-06-03T13:31:41.396213Z","shell.execute_reply.started":"2023-06-03T13:31:41.388906Z","shell.execute_reply":"2023-06-03T13:31:41.394894Z"}}},{"cell_type":"markdown","source":"## Model 4: Fine-tuning an existing model all of the data\n\nEnough talk about how fine-tuning a model usually works with more data, let's try it out.\n\nWe'll start by downloading the full version of our 10 food classes dataset.\n\n","metadata":{}},{"cell_type":"code","source":"# Download and unzip 10 classes of data with all images\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip \nunzip_data(\"10_food_classes_all_data.zip\")\n\n# Setup data directories\ntrain_dir = \"10_food_classes_all_data/train/\"\ntest_dir = \"10_food_classes_all_data/test/\"","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:13:57.617076Z","iopub.execute_input":"2023-06-06T14:13:57.617508Z","iopub.status.idle":"2023-06-06T14:14:05.615389Z","shell.execute_reply.started":"2023-06-06T14:13:57.617474Z","shell.execute_reply":"2023-06-06T14:14:05.614082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many images are we working with now?\nwalk_through_dir(\"10_food_classes_all_data\")","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:05.617459Z","iopub.execute_input":"2023-06-06T14:14:05.617881Z","iopub.status.idle":"2023-06-06T14:14:05.638639Z","shell.execute_reply.started":"2023-06-06T14:14:05.617843Z","shell.execute_reply":"2023-06-06T14:14:05.637657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup data inputs\nimport tensorflow as tf\nIMG_SIZE = (224, 224)\ntrain_data_10_classes_full = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n                                                                                 label_mode=\"categorical\",\n                                                                                 image_size=IMG_SIZE)\n\n# Note: this is the same test dataset we've been using for the previous modelling experiments\ntest_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n                                                                label_mode=\"categorical\",\n                                                                image_size=IMG_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:05.640157Z","iopub.execute_input":"2023-06-06T14:14:05.640576Z","iopub.status.idle":"2023-06-06T14:14:06.556192Z","shell.execute_reply.started":"2023-06-06T14:14:05.640544Z","shell.execute_reply":"2023-06-06T14:14:06.555291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oh this is looking good. We've got 10x more images in of the training classes to work with.\n\nThe **test dataset is the same** we've been using for our previous experiments.\n\nAs it is now, our `model_2` has been fine-tuned on 10 percent of the data, so to begin fine-tuning on all of the data and keep our experiments consistent, we need to revert it back to the weights we checkpointed after 5 epochs of feature-extraction.\n\nTo demonstrate this, we'll first evaluate the current `model_2`.","metadata":{}},{"cell_type":"code","source":"# Evaluate model (this is the fine-tuned 10 percent of data version)\nmodel_2.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:06.557990Z","iopub.execute_input":"2023-06-06T14:14:06.558317Z","iopub.status.idle":"2023-06-06T14:14:12.589477Z","shell.execute_reply.started":"2023-06-06T14:14:06.558286Z","shell.execute_reply":"2023-06-06T14:14:12.588615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are the same values as `results_fine_tune_10_percent`.","metadata":{"execution":{"iopub.status.busy":"2023-06-03T14:15:32.521727Z","iopub.execute_input":"2023-06-03T14:15:32.522304Z","iopub.status.idle":"2023-06-03T14:15:32.534928Z","shell.execute_reply.started":"2023-06-03T14:15:32.522267Z","shell.execute_reply":"2023-06-03T14:15:32.533470Z"}}},{"cell_type":"code","source":"results_fine_tune_10_percent","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:14:12.592383Z","iopub.execute_input":"2023-06-06T14:14:12.592685Z","iopub.status.idle":"2023-06-06T14:14:12.599628Z","shell.execute_reply.started":"2023-06-06T14:14:12.592660Z","shell.execute_reply":"2023-06-06T14:14:12.598770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(f\"TensorFlow version: {tf.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:16:15.598762Z","iopub.execute_input":"2023-06-06T14:16:15.599729Z","iopub.status.idle":"2023-06-06T14:16:15.604660Z","shell.execute_reply.started":"2023-06-06T14:16:15.599696Z","shell.execute_reply":"2023-06-06T14:16:15.603481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll revert the model back to the saved weights.","metadata":{}},{"cell_type":"code","source":"# Load model from checkpoint, that way we can fine-tune from the same stage the 10 percent data model was fine-tuned from\ncheckpoint_dir = \"ten_percent_model_checkpoints_weights\"\nlatest_weights = tf.train.latest_checkpoint(checkpoint_dir)\n\n# Note: As of TensorFlow 2.10.0+, this may error, it should work with TensorFlow 2.9.0\n# See the fix here: https://github.com/mrdbourke/tensorflow-deep-learning/issues/544\n# Load model from checkpoint, that way we can fine-tune from the same stage the 10 percent data model was fine-tuned from\nmodel_2.load_weights(checkpoint_path) # revert model back to saved weights","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:16:46.879694Z","iopub.execute_input":"2023-06-06T14:16:46.880672Z","iopub.status.idle":"2023-06-06T14:16:48.153573Z","shell.execute_reply.started":"2023-06-06T14:16:46.880621Z","shell.execute_reply":"2023-06-06T14:16:48.152268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After loading the weights, this should have gone down (no fine-tuning)\nmodel_2.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:17:37.062135Z","iopub.execute_input":"2023-06-06T14:17:37.062789Z","iopub.status.idle":"2023-06-06T14:17:43.744622Z","shell.execute_reply.started":"2023-06-06T14:17:37.062743Z","shell.execute_reply":"2023-06-06T14:17:43.743611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check to see if the above two results are the same (they should be)\nresults_10_percent_data_aug","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:17:43.746401Z","iopub.execute_input":"2023-06-06T14:17:43.748855Z","iopub.status.idle":"2023-06-06T14:17:43.755697Z","shell.execute_reply.started":"2023-06-06T14:17:43.748819Z","shell.execute_reply":"2023-06-06T14:17:43.754667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, the previous steps might seem quite confusing but all we've done is:\n1. Trained a feature extraction transfer learning model for 5 epochs on 10% of the data (with all base model layers frozen) and saved the model's weights using `ModelCheckpoint`.\n2. Fine-tuned the same model on the same 10% of the data for a further 5 epochs with the top 10 layers of the base model unfrozen.\n3. Saved the results and training logs each time.\n4. Reloaded the model from 1 to do the same steps as 2 but with all of the data.\n\nThe same steps as 2?\n\nYeah, we're going to fine-tune the last 10 layers of the base model with the full dataset for another 5 epochs but first let's remind ourselves which layers are trainable.","metadata":{}},{"cell_type":"code","source":"# Check which layers are tuneable in the whole model\nfor layer_number, layer in enumerate(model_2.layers):\n  print(layer_number, layer.name, layer.trainable)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:17:59.206509Z","iopub.execute_input":"2023-06-06T14:17:59.207028Z","iopub.status.idle":"2023-06-06T14:17:59.216201Z","shell.execute_reply.started":"2023-06-06T14:17:59.206992Z","shell.execute_reply":"2023-06-06T14:17:59.213620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can we get a little more specific?","metadata":{}},{"cell_type":"code","source":"# Check which layers are tuneable in the base model\nfor layer_number, layer in enumerate(base_model.layers):\n  print(layer_number, layer.name, layer.trainable)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:18:13.759030Z","iopub.execute_input":"2023-06-06T14:18:13.759401Z","iopub.status.idle":"2023-06-06T14:18:13.770673Z","shell.execute_reply.started":"2023-06-06T14:18:13.759370Z","shell.execute_reply":"2023-06-06T14:18:13.768794Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking good! The last 10 layers are trainable (unfrozen).\n\nWe've got one more step to do before we can begin fine-tuning.\n\nDo you remember what it is?\n\nI'll give you a hint. We just reloaded the weights to our model and what do we need to do every time we make a change to our models?\n\nRecompile them!\n\nThis will be just as before.","metadata":{}},{"cell_type":"code","source":"# Compile\nmodel_2.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(lr=0.0001), # divide learning rate by 10 for fine-tuning\n                metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:18:36.732551Z","iopub.execute_input":"2023-06-06T14:18:36.733436Z","iopub.status.idle":"2023-06-06T14:18:36.752379Z","shell.execute_reply.started":"2023-06-06T14:18:36.733392Z","shell.execute_reply":"2023-06-06T14:18:36.751429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Continue to train and fine-tune the model to our data\nfine_tune_epochs = initial_epochs + 5\n\nhistory_fine_10_classes_full = model_2.fit(train_data_10_classes_full,\n                                           epochs=fine_tune_epochs,\n                                           initial_epoch=history_10_percent_data_aug.epoch[-1],\n                                           validation_data=test_data,\n                                           validation_steps=int(0.25 * len(test_data)),\n                                           callbacks=[create_tensorboard_callback(\"transfer_learning\", \"full_10_classes_fine_tune_last_10\")])","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:18:41.767392Z","iopub.execute_input":"2023-06-06T14:18:41.767814Z","iopub.status.idle":"2023-06-06T14:22:32.081987Z","shell.execute_reply.started":"2023-06-06T14:18:41.767779Z","shell.execute_reply":"2023-06-06T14:22:32.080882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ðŸ”‘ **Note:** Training took longer per epoch, but that makes sense because we're using 10x more training data than before.\n\nLet's evaluate on all of the test data.","metadata":{}},{"cell_type":"code","source":"results_fine_tune_full_data = model_2.evaluate(test_data)\nresults_fine_tune_full_data","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:22:32.367255Z","iopub.execute_input":"2023-06-06T14:22:32.367703Z","iopub.status.idle":"2023-06-06T14:22:42.619623Z","shell.execute_reply.started":"2023-06-06T14:22:32.367642Z","shell.execute_reply":"2023-06-06T14:22:42.618443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice! It looks like fine-tuning with all of the data has given our model a boost, how do the training curves look?","metadata":{}},{"cell_type":"code","source":"# How did fine-tuning go with more data?\ncompare_historys(original_history=history_10_percent_data_aug,\n                 new_history=history_fine_10_classes_full,\n                 initial_epochs=5)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:22:42.621143Z","iopub.execute_input":"2023-06-06T14:22:42.621585Z","iopub.status.idle":"2023-06-06T14:22:43.195447Z","shell.execute_reply.started":"2023-06-06T14:22:42.621551Z","shell.execute_reply":"2023-06-06T14:22:43.194518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like that extra data helped! Those curves are looking great. And if we trained for longer, they might even keep improving.","metadata":{}},{"cell_type":"markdown","source":"## Viewing our experiment data on TensorBoard\n\nRight now our experimental results are scattered all throughout our notebook. If we want to share them with someone, they'd be getting a bunch of different graphs and metrics... not a fun time.\n\nBut guess what?\n\nThanks to the TensorBoard callback we made with our helper function `create_tensorflow_callback()`, we've been tracking our modelling experiments the whole time.\n\nHow about we upload them to TensorBoard.dev and check them out?\n\nWe can do with the `tensorboard dev upload` command and passing it the directory where our experiments have been logged.\n\n> ðŸ”‘ **Note:** Remember, whatever you upload to TensorBoard.dev becomes public. If there are training logs you don't want to share, don't upload them.","metadata":{}},{"cell_type":"code","source":"# # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n# # Upload TensorBoard dev records\n# !tensorboard dev upload --logdir ./transfer_learning \\\n#   --name \"Transfer learning experiments\" \\\n#   --description \"A series of different transfer learning experiments with varying amounts of data and fine-tuning\" \\\n#   --one_shot # exits the uploader when upload has finished","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:23:38.897684Z","iopub.execute_input":"2023-06-06T14:23:38.898151Z","iopub.status.idle":"2023-06-06T14:23:38.903504Z","shell.execute_reply.started":"2023-06-06T14:23:38.898113Z","shell.execute_reply":"2023-06-06T14:23:38.902321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we've uploaded the results to TensorBoard.dev we get a shareable link we can use to view and compare our experiments and share our results with others if needed.\n\nYou can view the original versions of the experiments we ran in this notebook here: https://tensorboard.dev/experiment/2O76kw3PQbKl0lByfg5B4w/\n\n> ðŸ¤” **Question:** Which model performed the best? Why do you think this is? How did fine-tuning go?\n\nTo find all of your previous TensorBoard.dev experiments using the command `tensorboard dev list`.","metadata":{}},{"cell_type":"markdown","source":"And if you want to remove a previous experiment (and delete it from public viewing) you can use the command:\n\n```\ntensorboard dev delete --experiment_id [INSERT_EXPERIMENT_ID_TO_DELETE]```","metadata":{}},{"cell_type":"code","source":"# Remove previous experiments\n# !tensorboard dev delete --experiment_id OUbW0O3pRqqQgAphVBxi8Q","metadata":{"execution":{"iopub.status.busy":"2023-06-06T14:23:54.143051Z","iopub.execute_input":"2023-06-06T14:23:54.143665Z","iopub.status.idle":"2023-06-06T14:23:54.148192Z","shell.execute_reply.started":"2023-06-06T14:23:54.143620Z","shell.execute_reply":"2023-06-06T14:23:54.147108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ›  Exercises\n\n1. Write a function to visualize an image from any dataset (train or test file) and any class (e.g. \"steak\", \"pizza\"... etc), visualize it and make a prediction on it using a trained model.\n2. Use feature-extraction to train a transfer learning model on 10% of the Food Vision data for 10 epochs using [`tf.keras.applications.EfficientNetB0`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0) as the base model. Use the [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback to save the weights to file.\n3. Fine-tune the last 20 layers of the base model you trained in 2 for another 10 epochs. How did it go?\n4. Fine-tune the last 30 layers of the base model you trained in 2 for another 10 epochs. How did it go?","metadata":{}},{"cell_type":"markdown","source":"## ðŸ“– Extra-curriculum\n\n* Read the [documentation on data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) in TensorFlow.\n* Read the [ULMFit paper](https://arxiv.org/abs/1801.06146) (technical) for an introduction to the concept of freezing and unfreezing different layers.\n* Read up on learning rate scheduling (there's a [TensorFlow callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) for this), how could this influence our model training?\n  * If you're training for longer, you probably want to reduce the learning rate as you go... the closer you get to the bottom of the hill, the smaller steps you want to take. Imagine it like finding a coin at the bottom of your couch. In the beginning your arm movements are going to be large and the closer you get, the smaller your movements become.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}